model_params:
  vocab_size: 65
  layers: 4
  embed_dim: 256
  n_heads: 8
  ff_dim: 512
  max_context: 256
train:
  lr_log_2: -12
  batch_size: 1
