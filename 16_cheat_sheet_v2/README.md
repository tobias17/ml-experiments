# Cheat Sheet

An experiment in using the full transformer, encoder and decoder, to perform language modeling.

This specific experiment is using english wikipedia as reference material to the model while it is training with the hopes that we can see improved performance both in terms of accuracy but also memory requirements.

# Initial Setup

Create an `env.json` with the following fields:
```json
{
   "dataset_root": "/datasets/fineweb_cheat_sheet"
}
```
The above is the values I am using on my tinybox, but they will be used by the following code to know about your environment.

# Prepping the Dataset

TODO

# Train the Model

TODO
